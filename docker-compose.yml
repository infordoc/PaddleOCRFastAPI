# ============================================
# Docker Compose for PaddleOCR 3.x FastAPI
# Optimized for Dokploy deployment
# ============================================
version: "3.8"

services:
  paddleocr-api:
    # Build from local Dockerfile (Dokploy will build this automatically)
    build:
      context: .
      dockerfile: Dockerfile
    
    # Image name for the built image
    image: paddleocrfastapi:latest
    
    # Container name
    container_name: paddleocr-api-v3
    
    # Restart policy
    restart: unless-stopped
    
    # Port mapping
    ports:
      - "8100:8000"  # FastAPI default port
    
    # Environment variables
    environment:
      # Timezone configuration
      - TZ=America/Sao_Paulo
      
      # OCR Language (ch=Chinese, en=English, pt=Portuguese, etc.)
      # See: https://github.com/PaddlePaddle/PaddleOCR/blob/main/doc/doc_en/multi_languages_en.md
      - OCR_LANGUAGE=pt
      
      # Debug mode (set to 1 to enable debug output)
      - OCR_DEBUG=0
      
      # Force CPU usage (GPU not supported in this build)
      - USE_GPU=false
      
      # Optional: Disable model source connectivity check for faster startup
      # Uncomment to skip model hoster connectivity checks
      # - PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK=True
    
    # Volume mounts
    volumes:
      # Model cache - PaddleOCR 3.x stores models here
      # This persists models between container restarts
      - paddleocr_models:/root/.paddleocr
      
      # PaddleX model cache for VL models
      # VL models (PaddleOCR-VL-1.5, PaddleOCR-VL) are stored here
      - paddlex_models:/root/.paddlex
      
      # Optional: Upload directory (uncomment if needed)
      # - ./uploads:/app/uploads
      
      # Optional: Output directory (uncomment if needed)
      # - ./output:/app/output
    
    # Resource limits (adjust based on your server capacity)
    # NOTE: VL models require more resources than traditional models
    deploy:
      resources:
        limits:
          cpus: '3.0'      # Maximum CPU cores
          memory: 8G       # Maximum RAM (VL models need ~4-6GB, traditional ~2-3GB)
        reservations:
          cpus: '0.5'      # Minimum CPU cores
          memory: 2G       # Minimum RAM
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Allow time for models to download on first run
    
    # Network configuration (optional)
    # networks:
    #   - paddleocr_network

# Persistent volumes
volumes:
  paddleocr_models:
    driver: local
    # Optional: specify driver options for better performance
    # driver_opts:
    #   type: none
    #   device: /path/on/host/paddleocr_models
    #   o: bind
  
  paddlex_models:
    driver: local
    # PaddleX models for VL (Vision-Language) models
    # VL models are larger (~2GB) and stored separately

# Optional: Custom network
# networks:
#   paddleocr_network:
#     driver: bridge
